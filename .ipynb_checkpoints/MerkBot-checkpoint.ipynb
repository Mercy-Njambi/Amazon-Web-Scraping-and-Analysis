{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding children <b>recursively</b> entails getting *their* grandchildren too, as _viable_ results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generalize this process for each card and collect:\n",
    "- the brand name\n",
    "- the model of the device\n",
    "- the memory storage\n",
    "- the screen/display size, and\n",
    "- the connector type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratings by <b>*features*</b>.\n",
    "> to be implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalizaing Review Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with page 1\n",
      "Obtained link for page 2\n",
      "**************************************************\n",
      "Try again in an hour! Bot is blocked\n",
      "Done with page 2\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone with page \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pageNum))\n\u001b[0;32m    141\u001b[0m pageNum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 142\u001b[0m nextPageUrl \u001b[38;5;241m=\u001b[39m \u001b[43mpaginationSpan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    143\u001b[0m                attrs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maria-label\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGo to next page, page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(pageNum)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m})[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    145\u001b[0m url \u001b[38;5;241m=\u001b[39m mainUrl \u001b[38;5;241m+\u001b[39m nextPageUrl\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pageNum \u001b[38;5;241m!=\u001b[39m targetPages:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "# Libraries needed\n",
    "import pandas as pd\n",
    "import requests_html, re, time\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "\n",
    "# General variables\n",
    "mainUrl = 'https://www.amazon.com'\n",
    "headers = ({'User-Agent':\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.18362',\n",
    "            'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "pageNum = 1\n",
    "url = \"\"\"\n",
    "      https://www.amazon.com/s?bbn=7072561011&rh=n%3A7072561011%2Cp_n_feature_twenty-\n",
    "      one_browse-bin%3A21596696011&dc&qid=1665390197&rnid=17881854011&ref=lp_7072561011_nr_p_n_feature_twenty-one_browse-bin_0\n",
    "      \"\"\"\n",
    "targetPages = 15\n",
    "data = pd.DataFrame(columns=['brand', 'model', 'memoryStorage','opSystem','display', 'reviews'])\n",
    "\n",
    "session = requests_html.HTMLSession()\n",
    "while pageNum < targetPages:\n",
    "    currentUrl = url\n",
    "    response = session.get(url= currentUrl, headers=headers)\n",
    "    \n",
    "    # getting the response content\n",
    "\n",
    "    primaryPageSoup = BeautifulSoup(markup=response.content, features='lxml')\n",
    "    \n",
    "    # finding the container \n",
    "\n",
    "    try:\n",
    "        parentDiv = primaryPageSoup.find(name=\"div\", \n",
    "                                         attrs={'class': 's-main-slot s-result-list s-search-results sg-row'})\n",
    "\n",
    "        # finding the card with phone titles\n",
    "\n",
    "        phoneDivs = parentDiv.findChildren(name=\"div\", attrs={'data-uuid': True}, recursive=False)\n",
    "\n",
    "        for phone in phoneDivs:\n",
    "\n",
    "            redirectUrl = phone.find(name='a')['href']\n",
    "\n",
    "            pageRedirect = session.get(mainUrl + redirectUrl, headers=headers)\n",
    "\n",
    "            redirectSoup = BeautifulSoup(markup=pageRedirect.content, features='lxml')\n",
    "\n",
    "            # Grabbing link to reviews\n",
    "            try:\n",
    "                reviewsUrl = redirectSoup.find(name=\"a\", \n",
    "                                               attrs={'data-hook': 'see-all-reviews-link-foot'})['href']\n",
    "            except TypeError:\n",
    "                # If no reviews found, no point in collecting the information\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Locate table with Device Information\n",
    "                infoTable = redirectSoup.find(name='table', attrs={'class': 'a-normal a-spacing-micro'})\n",
    "\n",
    "                # Gather Brand Information- eg, Samsung, Iphone, One Plus\n",
    "                brandRow = infoTable.find(name='tr', attrs={'class': 'a-spacing-small po-brand'})\n",
    "                brand = brandRow.findChildren(name='td', recursive=False)[1].text.strip()\n",
    "\n",
    "                # Gather Device Model Information - eg, Iphone XS, S20FE\n",
    "                modelRow = infoTable.find(name='tr', attrs={'class': 'a-spacing-small po-model_name'})\n",
    "                model = modelRow.findChildren(name='td', recursive=False)[1].text.strip()\n",
    "\n",
    "                # Gather Operating System Information - eg IOS, Android 13\n",
    "                opSystemrow = infoTable.find(name='tr', \n",
    "                                             attrs={'class': 'a-spacing-small po-operating_system'})\n",
    "                operatingSystem = opSystemrow.findChildren(name='td', recursive=False)[1].text.strip()\n",
    "\n",
    "                # Gather ROM information - eg, 64GB, 128GB\n",
    "                memoryStorageRow = infoTable.find(name='tr', \n",
    "                                                  attrs={'class': 'a-spacing-small po-memory_storage_capacity'})\n",
    "                memoryStorage = memoryStorageRow.findChildren(name='td', recursive=False)[1].text.strip()\n",
    "\n",
    "                # Gather Screen Display Size - eg, 5.8 Inches\n",
    "                displaySizeRow = infoTable.find(name='tr',\n",
    "                                                attrs={'class': 'a-spacing-small po-display.size'})\n",
    "                displaySize = displaySizeRow.findChildren(name=\"td\", recursive=False)[1].text.strip()\n",
    "\n",
    "            except AttributeError:\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Pagination for Reviews \n",
    "            pageCounter = 0\n",
    "\n",
    "            while pageCounter < 5:\n",
    "                # Getting request for the review page\n",
    "                reviewsPage = session.get(url=mainUrl + reviewsUrl,\n",
    "                                      headers=headers)\n",
    "                # Creating the Soup\n",
    "                reviewsSoup = BeautifulSoup(reviewsPage.content, features='lxml')\n",
    "\n",
    "                try: \n",
    "                    # Obtaining the reviews parent div\n",
    "                    reviewsDiv = reviewsSoup.find(name=\"div\", \n",
    "                                               attrs={'id': 'cm_cr-review_list'})\n",
    "\n",
    "                    # All reviews in their respective divs\n",
    "                    reviewsList = reviewsDiv.findChildren(name='div',\n",
    "                                                     attrs={'data-hook': 'review'},\n",
    "                                                     recursive=False)\n",
    "\n",
    "                    for div in reviewsList:\n",
    "                        text = div.find(name=\"span\", attrs={'data-hook': 'review-body'}).text.strip('\\n')\n",
    "\n",
    "                        # Append Information to csv, first five column entries will be repeated per review.\n",
    "                        data.loc[len(data.index)] = [brand, model, memoryStorage, operatingSystem, displaySize, text]\n",
    "                        \n",
    "                    try:\n",
    "                        # Getting next page url and updating the reviewsUrl variable\n",
    "                        paginationContainer = reviewsSoup.find(name=\"ul\", attrs={'class': 'a-pagination'})\n",
    "                        nextReviewsUrl = paginationContainer.findChildren(name=\"li\", recursive=False)[1].find(name=\"a\")['href']\n",
    "                        reviewsUrl = nextReviewsUrl\n",
    "                    except TypeError:\n",
    "                        break \n",
    "\n",
    "\n",
    "                except AttributeError:\n",
    "                    break\n",
    "\n",
    "                # Sleeping to avoid making too many requests\n",
    "                time.sleep(10)\n",
    "\n",
    "                # Updating the page counter\n",
    "                pageCounter += 1\n",
    "    \n",
    "    except AttributeError:\n",
    "        print(\"Try again in an hour! Bot is blocked\")\n",
    "        data.to_csv(\"./Data/amazon_data\", mode=\"a\",\n",
    "                   header=False, index=False)\n",
    "        break\n",
    "        \n",
    "    # Retrieve next Page Url\n",
    "    paginationSpan = primaryPageSoup.find(name='span', attrs={'class': 's-pagination-strip'})\n",
    "    \n",
    "    print(\"Done with page {}\".format(pageNum))\n",
    "    pageNum += 1\n",
    "    nextPageUrl = paginationSpan.find(name='a',\n",
    "                   attrs={'aria-label': f'Go to next page, page {str(pageNum)}'})['href']\n",
    "    \n",
    "    url = mainUrl + nextPageUrl\n",
    "    \n",
    "    if pageNum != targetPages:\n",
    "        print(\"Obtained link for page {}\".format(pageNum), end=\"\\n\" + \"*\" * 50 +\"\\n\")\n",
    "        time.sleep(30)\n",
    "    else:\n",
    "        print(\"Done scraping\", end=\"\\n\" + \"*\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sandbox",
   "language": "python",
   "name": "sandbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "9c94107dbb6f31e400bb19858c9768759ef7c8600537d1d879eb7d5b058e9ee3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
